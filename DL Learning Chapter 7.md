Chapter 7 深度学习中的正则化
===========

我们将正则化定义为 ‘‘对学习算法的修改——旨在减少泛化误差而不是训练误差’’。

## 7.1 参数范数惩罚

许多正则化方法通过对目标函数 J 添加一个参数范数惩罚 Ω(θ)，限制模型（如神经网络、线性回归或逻辑回归）的学习能力。我们将正则化后的目标函数记为˜J：˜J( θ ; X , y ) = J ( θ ; X , y ) + α Ω ( θ )

### 7.1.1 L <sup>2</sup> 参数正则化

最简单而又最常见的参数范数惩罚，即通常被称为权重衰减（weight decay）的 L 2 参数范数惩罚。这个正则化策略通过向目标函数添加一个正则项 Ω(θ) =1/2 ∥w∥<sub>2</sub><sup>2</sup> ，使权重更加接近原点1 。在其他学术圈，L 2 也被称为岭回归或 Tikhonov 正则。

我们可以看到，L 2 正则化能让学习算法 ‘‘感知’’ 到具有较高方差的输入 x，因此与输出目标的协方差较小（相对增加方差）的特征的权重将会收缩。

### 7.1.2 L <sup>1</sup> 参数正则化

一个选择是使用 L 1 正则化。形式地，对模型参数 w 的 L 1 正则化被定义为：Ω(θ) = ∥w∥ <sub>1</sub> =∑<sub>i</sub>|w <sub>i</sub> |

我们可以看到正则化对梯度的影响不再是线性地缩放每个 w i ；而是添加了一项与sign(w i ) 同号的常数。使用这种形式的梯度之后，我们不一定能得到 J(X,y;w) 二次近似的直接算术解（L 2 正则化时可以）。

相比 L 2 正则化，L 1 正则化会产生更稀疏（sparse）的解。此处稀疏性指的是最优值中的一些参数为 0。

由 L 1 正则化导出的稀疏性质已经被广泛地用于特征选择（feature selection）机制。特征选择从可用的特征子集选择出有意义的特征，化简机器学习问题。

### 7.1.3 L1 L2正则化总结

L2 regularizer ：使得模型的解偏向于范数较小的 W，通过限制 W 范数的大小实现了对模型空间的限制，从而在一定程度上避免了 overfitting 。不过 ridge regression 并不具有产生稀疏解的能力，得到的系数仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。

L1 regularizer ：它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。

## 7.2 作为约束的范数惩罚

有时候，我们希望使用显式的限制，而不是惩罚。我们可以修改下降算法（如随机梯度下降算法），使其先计算 J(θ) 的下降步，然后将 θ 投影到满足 Ω(θ) < k 的最近点。如果我们知道什么样的 k 是合适的，而不想花时间寻找对应于此 k 处的 α 值，这会非常有用。

另一个使用显式约束和重投影而不是使用惩罚强加约束的原因是惩罚可能会导致目标函数非凸而使算法陷入局部极小 (对应于小的 θ）。

最后，因为重投影的显式约束还对优化过程增加了一定的稳定性，所以这是另一个好处。当使用较高的学习率时，很可能进入正反馈，即大的权重诱导大梯度，然后使得权重获得较大更新。

这个部分有一些公式推导，包括一些理论上的东西，都是要用到凸优化的知识点的，然鹅当时没好好学太伤了，就等看啥时候有空再学一下吧。

## 7.3 正则化和欠约束问题

## 7.4 数据集增强

数据集增强对一个具体的分类问题来说是特别有效的方法：对象识别。图像是高维的并包括各种巨大的变化因素，其中有许多可以轻易地模拟。使模型已使用卷积和池化技术（第九章）对部分平移保持不变，沿训练图像每个方向平移几个像素的操作通常可以大大改善泛化。许多其他操作如旋转图像或缩放图像也已被证明非常有效。

数据集增强对语音识别任务也是有效的。在神经网络的输入层注入噪声也可以被看作是数据增强的一种方式。对于许多分类甚至一些回归任务而言，即使小的随机噪声被加到输入，任务仍应该是能够被解决的。

然而，神经网络被证明对噪声不是非常健壮。改善神经网络健壮性的方法之一是简单地将随机噪声添加到输入再进行训练。

## 7.5 噪声鲁棒性

在一般情况下，注入噪声远比简单地收缩参数强大，特别是噪声被添加到隐藏单元时会更加强大。

在某些假设下，施加于权重的噪声可以被解释为与更传统的正则化形式等同，鼓励要学习的函数保持稳定。

大多数数据集的 y 标签都有一定错误。避免这种情况的一种方法是显式地对标签上的噪声进行建模。

标签平滑（label smoothing）通过把确切分类目标从 0 和1 替换成ϵ/(k−1)和 1 − ϵ，正则化具有 k 个输出的 softmax 函数 的模型。标准交叉熵损失可以用在这些非确切目标的输出上。签平滑的优势是能够防止模型追求确切概率而不影响模型学习正确分类。

## 7.6 半监督学习

在深度学习的背景下，半监督学习通常指的是学习一个表示 h = f(x)。学习表示的目的是使相同类中的样本有类似的表示。无监督学习可以为如何在表示空间聚集样本提供有用线索。

## 7.7 多任务学习

多任务学习是通过合并几个任务中的样例（可以视为对参数施加的软约束）来提高泛化的一种方式。正如额外的训练样本能够将模型参数推向具有更好泛化能力的值一样，当模型的一部分被多个额外的任务共享时，这部分将被约束为良好的值（如果共享合理），通常会带来更好的泛化能力。

因为共享参数，其统计强度可大大提高（共享参数的样本数量相对于单任务模式增加的比例），并能改善泛化和泛化误差的范围。

## 7.8 提前终止

提前终止是一种非常不显眼的正则化形式，它几乎不需要改变基本训练过程、目标函数或一组允许的参数值。这意味着，无需破坏学习动态就能很容易地使用提前终止。相对于权重衰减，必须小心不能使用太多的权重衰减，以防网络陷入不良局部极小点(对应于病态的小权重)。

提前终止需要验证集，这意味着某些训练数据不能被馈送到模型。为了更好地利用这一额外的数据，我们可以在完成提前终止的首次训练之后，进行额外的训练。在第二轮，即额外的训练步骤中，所有的训练数据都被包括在内。有两个基本的策略都可以用于第二轮训练过程。

一个策略是再次初始化模型，然后使用所有数据再次训练。在这个第二轮训练过程中，我们使用第一轮提前终止训练确定的最佳步数。

另一个策略是保持从第一轮训练获得的参数，然后使用全部的数据继续训练。在这个阶段，已经没有验证集指导我们需要在训练多少步后终止。取而代之，我们可以监控验证集的平均损失函数，并继续训练，直到它低于提前终止过程终止时的目标值。此策略避免了重新训练模型的高成本，但表现并没有那么好。

## 7.9 参数绑定和参数共享

参数范数惩罚是正则化参数使其彼此接近的一种方式，而更流行的方法是使用约束：强迫某些参数相等。由于我们将各种模型或模型组件解释为共享唯一的一组参数，这种正则化方法通常被称为参数共享（parameter sharing）。和正则化参数使其接近（通过范数惩罚）相比，参数共享的一个显著优点是，只有参数（唯一一个集合）的子集需要被存储在内存中。

## 7.10 稀疏表示

L1惩罚是使表示稀疏的方法之一。其他方法还包括从表示上的Student-t 先验导出的惩罚和KL 散度惩罚，这些方法对于将表示中的元素约束于单位区间上特别有用。

还有一些其他方法通过激活值的硬性约束来获得表示稀疏。例如，正交匹配追踪还有一些其他方法通过激活值的硬性约束来获得表示稀疏。例如，正交匹配追踪 (orthogonal matching pursuit)。

## 7.11 Bagging 和其他集成方法

Bagging（bootstrap aggregating）是通过结合几个模型降低泛化误差的技术。主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例子，被称为模型平均（model averaging）。采用这种策略的技术被称为集成方法。模型平均奏效的原因是不同的模型通常不会在测试集上产生完全相同的误差

集成至少与它的任何成员表现得一样好，并且如果成员的误差是独立的，集成将显著地比其成员表现得更好。

Bagging涉及构造 k 个不同的数据集。每个数据集从原始数据集中重
复采样构成，和原始数据集具有相同数量的样例。

## 7.12 Dropout

Dropout提供了正则化一大类模型的方法，计算方便但功能强大。Dropout提供了一种廉价的Bagging集成近似，能够训练和评估指数级数量的神经网络。

## 7.13 对抗训练

在精度达到人类水平的神经网络上通过优化过程故意构造数据点，其上的误差率接近100%，模型在这个输入点 x ′ 的输出与附近的数据点 x 非常不同。在许多情况下，x ′ 与 x 非常近似，人类观察者不会察觉原始样本
和对抗样本（adversarial example）之间的差异，但是网络会作出非常不同的预测。

对抗训练通过鼓励网络在训练数据附近的局部区域恒定来限制这一高度敏感的局部线性行为。这可以被看作是一种明确地向监督神经网络引入局部恒定先验的方法。

## 7.14 总结

这些都算是一个引入式的程度吧，等到真的要用到的时候再去查资料吧。比如像dropout 对抗训练这种，明显水还挺深的。
