Nineth chapter in the book DeepLearning
=====

## 9.1 卷积运算

卷积公式：

s(t) = (x ∗ w)(t).

s(t) =∫x(a)w(t − a)da.

离散：

s(t) = (x ∗ w)(t) =∑x(a)w(t − a).

卷积的第一个参数（在这个例子中，函数 x）通常叫做输入（input），第二个参数（函数 w）叫做核函数（kernel function）。输出有时被称作特征映射（feature map）。

二维形式：

S( i,j ) = (I ∗ K) (i,j) = ∑m ∑n I( m,n ) K( i − m,j − n )

## 9.2 动机

卷积运算通过三个重要的思想来帮助改进机器学习系统：稀疏交互（sparse
interactions）、参数共享（parameter sharing）、等变表示（equivariant representations）。

### 9.2.1 稀疏交互

传统的神经网络使用矩阵乘法来建立输入与输出的连接关系。其中，参数矩
阵中`每一个`单独的参数都描述了一个输入单元与一个输出单元间的交互。

然而，卷积网络具有稀疏交互（sparse interactions）（也叫做稀疏连接（sparse connectivity）或者稀疏权重（sparse weights））的特征。这是使核的大小远小于输入的大小来达到的。

总的来说，就是用小的卷积核，可以让每个输入点只影响一定范围的输出点，如果直接矩阵相乘的话就会使输入的每一个点都影响输出的每一个点。另外，在深层的卷积也可以影响较多的输出点，eg，对于3 * 3的卷积核，每个输入点能影响5 * 5的输出，然后对于下一层来说，这25个点每个又能影响5 * 5，所以影响的就很多。同时，利用小的卷积核可以减少运算的复杂度。

### 9.2.2 参数共享

参数共享（parameter sharing）是指在一个模型的多个函数中使用相同的参数。在传统的神经网络中，当计算一层的输出时，权重矩阵的每一个元素只使用一次，当它乘以输入的一个元素后就再也不会用到了。

卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。

提高效率，减少参数量，否则可能会有非常大的参数量且值为0，就没有什么意义。

### 9.2.3 等变表示

如果一个函数满足输入改变，输出也以同样的方式改变这一性质，我们就说它是等变 (equivariant) 的。

特别地，如果函数 f(x) 与 g(x) 满足 f(g(x)) = g(f(x))，我们就说 f(x) 对于变换 g 具有等变性。对于卷积来说，如果令 g 是输入的任意平移函数，那么卷积函数对于 g 具有等变性。

如果我们移动输入中的对象，它的表示也会在输出中移动同样的量。

## 9.3 池化

卷积网络中一个典型层包含三级。在第一级中，这一层并行地计
算多个卷积产生一组线性激活响应。在第二级中，每一个线性激活响应将会通过一个非线性的激活函数，例如整流线性激活函数。这一级有时也被称为探测级（detectorstage）。在第三级中，我们使用池化函数（pooling function）来进一步调整这一层的输出。

池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。不管采用什么样的池化函数，当输入作出少量平移时，池化能够帮助输入的表示近似不变（invariant）。

因为池化综合了全部邻居的反馈，这使得池化单元少于探测单元成为可能。

## 9.4 卷积与池化作为一种无限强的先验

先验概率分布（prior probability distribution）是一个模型参数的概率分布，它刻画了在我们看到数据之前我们认为什么样的模型是合理的信念。

先验被认为是强或者弱取决于先验中概率密度的集中程度。弱先验具有较高的
熵值。这样的先验允许数据对于参数的改变具有或多或少的自由性。强先验具有较低的熵值。这样的先验在决定参数最终取值时起着更加积极的作用。

一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数赋值，无论数据对于这些参数的值给出了多大的支持。

我们可以把卷积网络类比成全连接网络，但对于这个全连接网络的权重有一个无限强的先验。这个无限强的先验是说一个隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动。这个先验也要求除了那些处在隐藏单元的小的空间连续的接受域内的权重以外，其余的权重都为零。总之，我们可以把卷积的使用当作是对网络中一层的参数引入了一个无限强的先验概率分布。这个先验说明了该层应该学得的函数只包含局部连接关系并且对平移具有等变性。类似的，使用池化也是一个无限强的先验：每一个单元都具有对少量平移的不变性。

这块没太懂..先放着好了...

## 9.5 基本卷积函数的变体

首先，当我们提到神经网络中的卷积时，我们通常是指由多个并行卷积组成的
运算。因为具有单个核的卷积只能提取一种类型的特征。

另外，输入通常也不仅仅是实值的网格，而是由一系列观测数据的向量构成的
网格。当处理图像时，我们通常把卷积的输入输出都看作是 3 维的张量，其中一个索引用于标明不同的通道（例如红绿蓝），另外两个索引标明在每个通道上的空间坐标。

我们有时会希望跳过核中的一些位置来降低计算的开销（相应的代价是提取特征没有先前那么好了）。我们可以把这一过程看作是对全卷积函数输出的下采样(downsampling)

有三种零填充设定的情况值得注意。

* 第一种是无论怎样都不使用零填充的极端情况，并且卷积核只允许访问那些图像中能够完全包含整个核的位置。在 MATLAB的术语中，这称为有效（valid）卷积。

* 第二种特殊的情况是只进行足够的零填充来保持输出和输入具有相同的大小。在 MATLAB 的术语中，这称为相同（same）卷积。这可能会导致边界像素存在一定程度的欠表示。

* 这使得第三种极端情况产生了，在 MATLAB 中称为全（full）卷积。它进行了足够多的零填充使得每个像素在每个方向上恰好被访问了 k 次，最终输出图像的宽度为 m + k − 1。

* 通常零填充的最优数量（对于测试集的分类正确率）处于 “有效卷积’’ 和“相同卷积’’ 之间的某个位置。

总的来说第9.5还有一些东西，但是有点复杂..没太懂而且不好摘抄...

## 9.6 结构化输出

没懂重点是啥。

## 9.7 数据类型

卷积网络的一个优点是它们还可以处理具有可变的空间尺度的输入。这些类型的输入不能用传统的基于矩阵乘法的神经网络来表示。

有时，网络的输出允许和输入一样具有可变的大小，例如如果我们想要为输入的每个像素分配一个类标签。在这种情况下，不需要进一步的设计工作。在其他情况下，网络必须产生一些固定大小的输出。

注意，使用卷积处理可变尺寸的输入，仅对输入是因为包含对同种事物的不同量的观察 (时间上不同长度的记录，空间上不同宽度的观察等) 而导致的尺寸变化这种情况才有意义。如果输入是因为它可以选择性地包括不同种类的观察而具有可变尺寸，使用卷积是不合理的。

## 9.8 高效的卷积算法

卷积等效于使用傅立叶变换将输入与核都转换到频域、执行两个信号的逐点相乘，再使用傅立叶逆变换转换回时域。对于某些问题的规模，这种算法可能比离散卷积的朴素实现更快。

当一个 d 维的核可以表示成 d 个向量（每一维一个向量）的外积时，该核被称为可分离的（separable）。当核可分离时，朴素的卷积是低效的。它等价于组合 d 个一维卷积，每个卷积使用这些向量中的一个。组合方法显著快于使用它们的外积来执行一个 d 维的卷积。

## 9.9 随机或无监督的特征

当使用梯度下降执行监督训练时，每步梯度计算需要完整地运行整个网络的前向传播和反向传播。减少卷积网络训练成本的一种方式是使用那些不是由监督方式训练得到的特征。

有三种基本策略可以不通过监督训练而得到卷积核。其中一种是简单地随机初始化它们。另一种是手动设计它们，例如设置每个核在一个特定的方向或尺度来检测边缘。最后，可以使用无监督的标准来学习核。

随机过滤器经常在卷积网络中表现得出乎意料得好，由卷积和随后的池化组成的层，当赋予随机权重时，自然地变得具有频率选择性和平移不变性。他们认为这提供了一种廉价的方法来选择卷积网络的结构：首先通过仅训练最后一层来评估几个卷积网络结构的性能，然后选择最好的结构并使用更昂贵的方法来训练整个网络。

一个中间方法是学习特征，但是使用那种不需要在每个梯度计算步骤中都进行完整的前向和反向传播的方法。与多层感知机一样，我们使用贪心逐层预训练，单独训练第一层，然后一次性地从第一层提取所有特征，之后用那些特征单独训练第二层。

