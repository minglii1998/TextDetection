Chapter 6 深度前馈网络
======

深度前馈网络（deep feedforward network），也叫作前馈神经网络（feedforward neural network）或者多层感知机（multilayer perceptron, MLP），是典型的深度学习模型。前馈网络的目标是近似某个函数 f <sup>∗</sup> 。

这种模型被称为前向（feedforward）的，是因为信息流过 x 的函数，流经用于定义 f 的中间计算过程，最终到达输出 y。这种模型被称为前向（feedforward）的，是因为信息流过 x 的函数，流经用于定义 f 的中间计算过程，最终到达输出 y。

##6.1 实例： 学习 XOR

XOR 函数（‘‘异或’’ 逻辑）是两个二进制值 x 1 和 x 2 的运算。当这些二进制值中恰好有一个为 1 时，XOR 函数返回值为 1。其余情况下返回值为 0。

在训练XOR函数时，无法使用先行函数，因为x1=0时，x2=1时值为1，x2=0时值为0，说明当x1=0时值随x2的增大而增大；而当x1=1是，情况更好相反。说明x2的系数是与x1有关的。然而在线性函数中，x2的系数是固定的，因此不能用单纯的线性函数。

因此在输入和输出层中加一个隐藏层。

整流线性激活函数用于线性变换的输出将产生非线性变换。由于整流线性单元几乎是线性的，因此它们保留了许多使得线性模型易于使用基于梯度的方法进行优化的属性。它们还保留了许多使得线性模型能够泛化良好的属性。

在这个栗子中看到了relu函数的神奇功效，把原本的一个负值变为了0后，原来的线性函数就变成非线性的了，就使得出现了正解。

## 6.2 基于梯度的学习

我们到目前为止看到的线性模型和神经网络的最大区别，在于神经网络的非线性导致大多数我们感兴趣的代价函数都变得非凸。凸优化从任何一种初始参数出发都会收敛。用于非凸损失函数的随机梯度下降没有这种收敛性保证，并且对参数的初始值很敏感。

### 6.2.1 代价函数

这里提到了一个正则化，在知乎上看到了一个很好的解释。[机器学习中常常提到的正则化到底是什么意思？](https://www.zhihu.com/question/20924039)高赞回答说得蛮清楚的了，我再用自己的话稍微归纳一下吧。

正则化是为了防止过拟合。一般来说过拟合的原因是x项太多，因此为了防止过拟合最好就是让x项不要呢么多，那怎么做到呢？直接减少x肯定不行，那就减少w咯，因为当w<sub>i</sub>=0时，那个x<sub>i</sub>自然也就没了。

因此，选择用0范数来限定w的维度，然而0范数不好计算，因此便用了1范数，但是后来发现2范数在w小的时候更接近0更好用，然后就变成大多用2范数了。所以顺其自然的正则化就是我们将结构风险最小化的过程，它们是等价的。

#### 6.2.1.1 使用最大似然学习条件分布

大多数现代的神经网络使用最大似然来训练。这意味着代价函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价。这个代价函数表示为:

J(θ) = −E <sub>x,y∼ˆ p data</sub> log p <sub>model</sub> (y | x). 

使用最大似然来导出代价函数的方法的一个优势是，它减轻了为每个模型设计代价函数的负担。明确一个模型 p(y | x) 则自动地确定了一个代价函数 logp(y | x)。

用于实现最大似然估计的交叉熵代价函数有一个不同寻常的特性，那就是当它被应用于实践中经常遇到的模型时，它通常没有最小值。

#### 6.2.1.2 学习条件统计量

不知道在讲啥..告辞

### 6.2.2 输出单元

代价函数的选择与输出单元的选择紧密相关。大多数时候，我们简单地使用数据分布和模型分布间的交叉熵。选择如何表示输出决定了交叉熵函数的形式

我佛了，我不知道这个部分是在干啥，每个字都能读懂但是何在一起硬是不知道这是在干啥，佛了佛了先不管了。

## 6.3 隐藏单元

现在我们转向一个前馈神经网络独有的问题：该如何选择隐藏单元的类型，这些隐藏单元用在模型的隐藏层中。

我们这里描述对于每种隐藏单元的一些基本直觉。这些直觉可以用来建议我们何时来尝试一些单元。通常不可能预先预测出哪种隐藏单元工作得最好。

除非另有说明，大多数的隐藏单元都可以描述为接受输入向量 x，计算仿射变换 z = W <sup>⊤</sup> x + b，然后使用一个逐元素的非线性函数 g(z)。大多数隐藏单元的区别仅仅在于激活函数 g(z) 的形式。

整流线性单元的一个缺陷是它们不能通过基于梯度的方法学习那些使它们激活为零的样本。

sigmoid 单元在其大部分定义域内都饱和——当 z 取绝对值很大的正值时，它们饱和到一个高值，当 z 取绝对值很大的负值时，它们饱和到一个低值，并且仅仅当 z 接近 0 时它们才对输入强烈敏感。sigmoid 单元的广泛饱和性会使得基于梯度的学习变得非常困难。

## 6.4 架构设计

大多数神经网络被组织成称为层的单元组。大多数神经网络架构将这些层布置成链式结构，其中每一层都是前一层的函数。

### 6.4.1 万能近似性质和深度

具有隐藏层的前馈网络提供了一种万能近似框架。具体来说，万能近似定理表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种 ‘‘挤压’’ 性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的 Borel 可测函数。前馈网络的导数也可以任意好地来近似函数的导数。

神经网络也可以近似从任何有限维离散空间映射到另一个的任意函数。虽然原始定理最初以具有特殊激活函数的单元的形式来描述，这个激活函数当变量取绝对值非常大的正值和负值时都会饱和，万能近似定理也已经被证明对于更广泛类别的激活函数也是适用的，其中就包括现在常用的整流线性单元。

万能近似定理意味着无论我们试图学习什么函数，我们知道一个大的 MLP 一定能够表示这个函数。然而，我们不能保证训练算法能够学得这个函数。

### 6.4.2 其他架构上的考虑

目前为止，我们都将神经网络描述成层的简单链式结构，主要的考虑因素是网络的深度和每层的宽度。在实践中，神经网络显示出相当的多样性。

## 6.5 反向传播和其他的微分算法

在训练过程中，前向传播可以持续向前直到它产生一个标量代价函数 J(θ)。反向传播（back propagation）算法，经常简称为backprop，允许来自代价函数的信息通过网络向后流动，以便计算梯度。

反向传播这个术语经常被误解为用于多层神经网络的整个学习算法。实际上，反向传播仅指用于计算梯度的方法，而另一种算法，例如随机梯度下降，使用该梯度来进行学习。此外，反向传播经常被误解为仅适用于多层神经网络，但是原则上它可以计算任何函数的导数（对于一些函数，正确的响应是报告函数的导数是未定义的）。

## 6.6 总结

6.5之后没有仔细地记录了，剩下的就偏向数学那边了，我就不再过多的记录了，如果真的需要用到的话再看吧。
